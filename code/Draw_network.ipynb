{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81e8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_multilabel_classification\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from NN_Models import *\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ee255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load('data/X_author_train.pt')\n",
    "y_train = torch.load('data/y_train.pt')\n",
    "# X_train = torch.load('data/X_author_embedding_train.pt')\n",
    "# y_train = torch.load('data/y_train.pt')\n",
    "\n",
    "X_test = torch.load('data/X_author_test.pt')\n",
    "y_test = torch.load('data/y_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71bdd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6268, 21146])\n",
      "torch.Size([3086, 21146])\n"
     ]
    }
   ],
   "source": [
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e1a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data loader\n",
    "train_dataset, train_dataloader = BinaryDataLoader(X_train, y_train, shuffle=True, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d895bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "model.set_dim(num_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7260c",
   "metadata": {},
   "source": [
    "# coauthor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cfcb3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetworkCoauthor(\n",
      "  (fc1): Linear(in_features=21146, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.coauthor_model())\n",
    "coauthor_model = model.coauthor_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a4ab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([512, 21146])\n"
     ]
    }
   ],
   "source": [
    "params = list(coauthor_model.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8633a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4777, 0.4748, 0.5093,  ..., 0.4955, 0.5417, 0.5120],\n",
      "        [0.5115, 0.4595, 0.4980,  ..., 0.5284, 0.5326, 0.5250],\n",
      "        [0.4770, 0.4539, 0.5313,  ..., 0.5305, 0.5434, 0.5135],\n",
      "        ...,\n",
      "        [0.5078, 0.4655, 0.5016,  ..., 0.5168, 0.5315, 0.5181],\n",
      "        [0.4724, 0.5049, 0.5156,  ..., 0.5454, 0.5109, 0.5004],\n",
      "        [0.5110, 0.4549, 0.5006,  ..., 0.4910, 0.5461, 0.5436]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'coauthor_model_network.pdf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(512,21146)\n",
    "out = coauthor_model(input)\n",
    "print(out)\n",
    "g = torchviz.make_dot(out)\n",
    "g.view(filename=\"coauthor_model_network\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77156f",
   "metadata": {},
   "source": [
    "# year_venue_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25d9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetworkYearVenue(\n",
      "  (fc1): Linear(in_features=21146, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.year_venue_model())\n",
    "year_venue_model = model.year_venue_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a71fc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([128, 21146])\n"
     ]
    }
   ],
   "source": [
    "params = list(year_venue_model.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ccfafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5093, 0.5226, 0.5921,  ..., 0.5564, 0.5126, 0.4578],\n",
      "        [0.6112, 0.4902, 0.4647,  ..., 0.5063, 0.5106, 0.4156],\n",
      "        [0.5641, 0.4747, 0.4517,  ..., 0.4678, 0.4582, 0.5470],\n",
      "        ...,\n",
      "        [0.6187, 0.4447, 0.4676,  ..., 0.4665, 0.5354, 0.4773],\n",
      "        [0.5354, 0.5383, 0.5670,  ..., 0.5507, 0.4377, 0.4642],\n",
      "        [0.5503, 0.5214, 0.4945,  ..., 0.5327, 0.3455, 0.4518]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'year_venue_model_network.pdf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(128,21146)\n",
    "out = year_venue_model(input)\n",
    "print(out)\n",
    "g = torchviz.make_dot(out)\n",
    "g.view(filename=\"year_venue_model_network\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9e0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
